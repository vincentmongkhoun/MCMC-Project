{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "present-jacket",
   "metadata": {},
   "source": [
    "Following ``Variational Inference: A Review for Statisticians, Blei et al; (2017)``, consider a Bayesian mixture of unit-variance univariate Gaussians. \n",
    "\n",
    "There are $K$ mixture components, each component $k$ of the mixture is a Gaussian distribution with mean $\\mu_k$ and variance 1. The random variables $\\mu = (\\mu_k)_{1\\leqslant k \\leqslant K}$ are assumed to be independent and identically distributed (i.i.d.) with Gaussian distribution with mean 0 and variance $\\sigma^2$. The weight of each mixture component $k$ written $\\omega_k$. Conditionally on $\\mu$, the observations $(X_i)_{1\\leqslant i\\leqslant n}$ are assumed to be i.i.d. with probability density:\n",
    "\n",
    "$$\n",
    "p(x|\\mu) = \\sum_{k=1}^K \\omega_k \\varphi_{\\mu_k,1}(x)\\,,\n",
    "$$\n",
    "\n",
    "where $\\varphi_{\\mu_k,\\sigma^2}$ is the Gaussian probability function with mean $\\mu_k$ and variance $\\sigma^2$. The likelihood is then given by:\n",
    "\n",
    "$$\n",
    "p(x_1,\\cdots,x_n) = \\int p(x_1,\\cdots,x_n|\\mu) p(\\mu) \\mathrm{d} \\mu = \\int \\prod_{i=1}^n p(x_i|\\mu) p(\\mu) \\mathrm{d} \\mu = \\int \\prod_{i=1}^n \\left(\\sum_{k=1}^K \\omega_k \\varphi_{\\mu_k,1}(x_i)\\right) p(\\mu) \\mathrm{d} \\mu\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-complaint",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "damaged-portuguese",
   "metadata": {},
   "source": [
    "Our aim is to approximate the posterior distribution $p(\\mu,c|x)$ where $c = (c_1,\\cdots,c_n)$ are the mixture components of the observations.  The mean-field variational family is described as follows:\n",
    "\n",
    "$$\n",
    "q(\\mu,c) = \\prod_{k=1}^K \\varphi_{m_k,s_k}(\\mu_k)\\prod_{i=1}^n \\mathrm{Cat}_{\\phi_i}(c_i)\\,, \n",
    "$$\n",
    "\n",
    "which means that:\n",
    "\n",
    "- $\\mu$ and $c$ are independent.\n",
    "- $(\\mu_{k})_{1\\leqslant k \\leqslant K}$ are independent with Gaussian distribution with means $(m_{k})_{1\\leqslant k \\leqslant K}$ and covariances $(\\Sigma _{k})_{1\\leqslant k \\leqslant K}$.\n",
    "\n",
    "\n",
    "\n",
    "- $(c_{i})_{1\\leqslant i \\leqslant n}$ are independent with multinomial distribution with parameters $(\\phi_i)_{1\\leqslant i \\leqslant n}$: $q(c_i=k) = \\phi_i(k)$ for $1\\leqslant k \\leqslant K$. \n",
    "\n",
    "The family of such distributions is parameterized by $(m_{k})_{1\\leqslant k \\leqslant K}\\in \\mathbb{R}^{d \\times K}$, the covariance matrices $(\\Sigma_{k})_{1\\leqslant k \\leqslant K}\\in (\\mathbb{S_d(R)}_+)^K$ and $(\\phi_i)_{1\\leqslant i \\leqslant n}\\in \\mathcal{S}_K^n$ where $\\mathcal{S}_K$ is the $K$-dimensional probability simplex. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-invitation",
   "metadata": {},
   "source": [
    "<font color=darkred>The objective function.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-harvest",
   "metadata": {},
   "source": [
    "The objective is now to find the\n",
    "\"best candidate\" in $\\mathcal{D}$ to approximate $p(\\mu,c|x)$, i.e. the one ``which minimizes the following KL divergence``:\n",
    "\n",
    "$$\n",
    "q^* = \\mathrm{Argmin}_{q\\in\\mathcal{D}} \\mathrm{KL}\\left(q(\\mu,c)\\|p(\\mu,c|x)\\right)\\,.\n",
    "$$\n",
    "\n",
    "Note that\n",
    "\\begin{align*}\n",
    "\\mathrm{KL}\\left(q(\\mu,c)\\|p(\\mu,c|x)\\right) &= \\mathbb{E}_q[\\log q(\\mu,c)] - \\mathbb{E}_q[\\log p(\\mu,c|x)]\\,,\\\\\n",
    " &= \\mathbb{E}_q[\\log q(\\mu,c)] - \\mathbb{E}_q[\\log p(\\mu,c,x)]+\\log p(x)\\,,\\\\\n",
    "&= -\\mathrm{ELBO}(q)+\\log p(x)\\,,\n",
    "\\end{align*}\n",
    "\n",
    "where the ``Evidence Lower Bound`` (ELBO) is\n",
    "\n",
    "$$\n",
    "\\mathrm{ELBO}(q) = -\\mathbb{E}_q[\\log q(\\mu,c)] + \\mathbb{E}_q[\\log p(\\mu,c,x)] \\,.\n",
    "$$\n",
    "\n",
    "Therefore, ``minimizing the KL divergence`` boils down to maximizing the ELBO, where $\\log p(x)\\geqslant \\mathrm{ELBO}(q)$.\n",
    "\n",
    "The complexity of the family $\\mathcal{D}$ determines the complexity of the optimization.\n",
    "\n",
    "By the way, the $\\mathrm{ELBO}$ function only depends on the parameters of $\\mathrm{q}$. That being said, we can write: \n",
    "$$\\mathrm{ELBO}(q) = \\mathrm{ELBO}((m_k)_k, (\\Sigma_k)_k, (\\phi_i(k))_{i,k})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-huntington",
   "metadata": {},
   "source": [
    "<font color=darkred>Computation of the $\\mathrm{ELBO}$.</font>\n",
    "\n",
    "We have \n",
    "$\n",
    "\\mathrm{ELBO}(q) = \\mathrm{ELBO}(m, s^{2}, \\phi)  = -\\mathbb{E}_q[\\log q(\\mu,c)] + \\mathbb{E}_q[\\log p(\\mu,c,x)] \\,.\n",
    "$\n",
    "\n",
    "Avec \n",
    "$\\mu_k \\sim \\mathcal{N}(m_k,\\Sigma_k)\\$ and \n",
    "\n",
    "\\begin{align*}\n",
    "\\varphi_{m_k,s_k}(\\mu_k) &= (\\det(2\\pi\\Sigma_k)^{\\frac{-1}{2}} \\exp(-\\frac{(\\mu_k - m_k)^{T} Σ_k^{-1} (\\mu_k - m_k)}{2})\\,,\\\\\n",
    "&= (2\\pi)^{-d}  (\\det(\\Sigma_k)^{\\frac{-1}{2}} \\exp(-\\frac{(\\mu_k - m_k)^{T} Σ_k^{-1} (\\mu_k - m_k)}{2})\n",
    "\\end{align*}\n",
    "$d$ being the feature dimension of our data.\n",
    "\n",
    "\n",
    "1.  First of all we have:\n",
    "\\begin{align*}\n",
    "q(\\mu,c) &= \\prod_{k=1}^K \\varphi_{m_k,s_k}(\\mu_k)\\prod_{i=1}^n \\mathrm{Cat}_{\\phi_i}(c_i)\\,,\\\\ \n",
    "  &= \\prod_{k=1}^K \\varphi_{m_k,s_k}(\\mu_k) \\prod_{i=k}^n  \\prod_{i=1}^K{\\phi_i}(c_i = k)^{\\mathbb{1}_{c_i = k}}\\,,\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Same for $p(\\mu,c,x)$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mu,c,x) &= p(\\mu) \\prod_{i=1}^n p(x_i|\\mu)\\,,\\\\ \n",
    "&= \\prod_{k=1}^Kp(\\mu_k) \\prod_{i=1}^n \\prod_{k=1}^K p(x_i|\\mu_k)^{\\mathbb{1}_{c_i = k}}\n",
    "  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "And so we have: \n",
    "\\begin{align*}\n",
    "\\log q(\\mu,c) &= \\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{1}_{c_i = k} \\log({\\phi_i}(k)) + \\sum_{k=1}^K - \\frac{\\log(\\det(\\Sigma_k)}{2} - \\frac{(\\mu_k - m_k)^{T} Σ_k^{-1} (\\mu_k - m_k)}{2} + \\mathrm{cst}\\,,\\\\ \n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{align*}\n",
    "\\log p(x, \\mu,c) &= \\sum_{i=1}^n \\sum_{k=1}^K  \\mathbb{1}_{c_i = k} \\left[- \\frac{\\log(\\det(I))}{2} - \\frac{(x_i - \\mu_k)^{T} (x_i - \\mu_k)}{2} \\right] \\\\ \n",
    "&+ \\sum_{k=1}^{K} -\\frac{\\log(\\det (\\Sigma))}{2} - \\frac{\\mu_k^{T}\\Sigma^{-1}\\mu_k}{2} + \\mathrm{cst} \\\\ \n",
    "\\end{align*}\n",
    "\n",
    "2. Knowing that \n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{\\mathcal{N}(m_k,\\Sigma_k)} ((\\mu_k - m_k)^{T} Σ_k^{-1} (\\mu_k - m_k)\n",
    "&= \\mathbb{E}(\\mathrm{Tr}(Σ_k^{-1} (\\mu_k - m_k)(\\mu_k - m_k)^{T})) \\\\ \n",
    "&= \\mathrm{Tr}(Σ_k^{-1} \\mathbb{E} ((\\mu_k - m_k)(\\mu_k - m_k)^{T} ))\\\\\n",
    "&= \\mathrm{Tr}(Σ_k^{-1} Σ_k) = d\n",
    "\\end{align*}\n",
    ",\n",
    "\\begin{align*}\n",
    "\\mathbb{E} \\left[ (x_i - \\mu_k)^{T} (x_i - \\mu_k) \\right] &=  \\mathbb{E} \\left[ \\mu_k^{T}\\mu_k -2\\mu_k^{T}x_i + x_i^{T}x_i \\right]\\,\\\\\n",
    "&= \\mathbb{E} \\left[ \\mu_k^{T}\\mu_k \\right] -2m_k^{T}x_i +\\mathbb{cst}\\,\\\\ \n",
    "&= m_k^{T}m_k + \\mathrm{Tr}(\\Sigma_k) -2m_k^{T}x_i + \\mathbb{cst}\n",
    "\\end{align*}\n",
    "\n",
    "and finally \n",
    "\\begin{align*}\n",
    "\\mathbb{E} \\left[ \\mu_k^{T}\\Sigma^{-1}\\mu_k \\right] &= \\mathbb{E} \\left[ \\mathrm{Tr} (\\mu_k^{T}\\Sigma^{-1}\\mu_k) \\right] \\,\\\\ \n",
    "&= \\mathbb{E} \\left[  (\\Sigma^{-1}\\mu_k \\mu_k^{T}) \\right] \\,\\\\ \n",
    "&= \\mathrm{Tr} ( \\Sigma^{-1} \\mathbb{E} (\\mu_k \\mu_k^{T}) \\left[  \\right])\\,\\\\ \n",
    "&= \\mathrm{Tr} ( \\Sigma^{-1} (m_k m_k^{T} + \\Sigma_{k}))\\,\\\\\n",
    "&=  m_k\\Sigma^{-1}m_k^{T} + \\mathrm{Tr} (\\Sigma^{-1} \\Sigma_{k})\n",
    "\\end{align*}\n",
    "\n",
    "So finally \n",
    "\\begin{align*}\n",
    "\\mathrm{ELBO}(q) &= \\mathrm{ELBO}(m, s^{2}, \\phi)\\,\\\\ \n",
    "&= \\sum_{k=1}^{K}  \\frac{\\log(\\det(\\Sigma_k))}{2}  - \\frac{m_k\\Sigma^{-1}m_k^{T} + \\mathrm{Tr} (\\Sigma^{-1} \\Sigma_{k})}{2} \\,\\\\\n",
    "&+ \\sum_{i=1}^n \\sum_{k=1}^K - \\phi_i(k) \\log(\\phi_i(k)) - \\phi_i(k) (\\frac{m_k^{T}m_k + \\mathrm{Tr}(\\Sigma_k) -2m_k^{T}x_i}{2})\\,\\\\ \n",
    "&= \\sum_{k=1}^{K}  \\frac{\\log(\\det(\\Sigma_k))}{2}  - \\frac{m_k\\Sigma^{-1}m_k^{T} + \\mathrm{Tr} (\\Sigma^{-1} \\Sigma_{k})}{2}\\,\\\\\n",
    "&+ \\sum_{i=1}^n \\sum_{k=1}^K \\phi_i(k) \\left[ m_k^{T}x_i - \\log(\\phi_i(k)) - \\frac{m_k^{T}m_k + \\mathrm{Tr}(\\Sigma_k)}{2}\\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-pontiac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "natural-identity",
   "metadata": {},
   "source": [
    "<font color=darkred> Proof of update equations in general with CAVI</font>\n",
    "\n",
    "\n",
    "The general the objective fonction that we want to monitor is the following $ \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{ELBO}(q) &= -\\mathbb{E}_q[\\log q(z)] + \\mathbb{E}_q[\\log p(z,x)] \\,\n",
    "\\end{align*}\n",
    "\n",
    "The premise is to find the distribution q that maximizes the $\\mathrm{ELBO}$. In this general without further hypothesis, finding the maximizer of this function is difficult. In pratice a family of q $\\mathrm{Q}$ is chosen.\n",
    "\n",
    "In this section, we focus on the mean-field variational family,\n",
    "where the latent variables are mutually independent and each\n",
    "governed by a distinct factor in the variational density. \n",
    "A member of this family is like \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{q}(z)&= \\prod_{j=1}^{m} q_j(z_j)\n",
    "\\end{align*}\n",
    "where m is the dimension of the hidden variable $z$ and $z_j \\sim q_j(z_j)$.\n",
    "\n",
    "One of the most commonly used algorithms for solving this optimization problem, coordinate ascent vari\u0002ational inference (CAVI) (Bishop 2006). CAVI iteratively opti\u0002mizes each factor of the mean-field variational density, while\n",
    "holding the others fixed.\n",
    "\n",
    "- Suppose that all $q_m$ are fixed expect $q_j$ and lets try to optimize the $\\mathrm(q)$ and other distributions  $(q_m)_{m \\neq j} $ fixed.\n",
    "Then the $\\mathrm{ELBO}$ is as the following:\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{ELBO}(q) &= \\mathrm{ELBO}(q_j)\\,,\\\\ \n",
    "&=  \\mathbb{E}_q \\left[ \\log p(z_j| x, z_{-j}) + \\log p( x, z_{-j}) - \\sum_{k=1}^{m} \\log q_j(z_j) \\right] \\,,\\\\ \n",
    "&= \\mathbb{E}_{q_j} \\left[ \n",
    "  \\mathbb{E}_{X, Z_{-j}} \\left[ \\log p(z_j| x, z_{-j}) + \\log p( x, z_{-j}) - \\sum_{k=1}^{m} \\log q_j(z_j) | z_j \\right] \\right]\\,\\\\ \n",
    "&= \\mathbb{E}_{q_j} \\left[  \\mathbb{E}_{X, Z_{-j}} \\left[ f(q_j(z_j) , \\log p(z_j| x, z_{-j})) | z_j \\right] \\right]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "$\\underset{\\rm q_{j}}{\\rm \\max\\mathbb{E}(q_j)} \\Leftrightarrow \\rm \\max\\mathbb{E}_{X, Z_{-j}} \\left[ f(q_j(z_j) , \\log p(z_j| x, z_{-j})) | z_j \\right])$\n",
    "\n",
    "So the maximizer $q^*_{j}$ verify \n",
    "\\begin{align*}\n",
    "q^*_{j}(z_j) &= \\arg \\max {E}_{X, Z_{-j}} \\left[ f(q_j(z_j) , \\log p(z_j| x, z_{-j})) | z_j \\right])\\,\\\\ \n",
    "&= \\arg \\max {E}_{X, Z_{-j}} \\left[ -  \\log q_j(z_j) +  \\log p(z_j| x, z_{-j}) \\right]\\,\\\\ \n",
    "&= -  \\log q_j(z_j) + {E}_{X, Z_{-j}} \\left[ \\log p(z_j| x, z_{-j})\\right] + \\mathbb{cst} \n",
    "\\end{align*}\n",
    "\n",
    "So we have $q^*_{j}(z_j)$ verify the following equation \n",
    "\n",
    "$$\\frac{-\\partial \\log q^*_j(z_j)}{\\partial q_j(z_j)} + \\frac{\\partial {E}_{X, Z_{-j}} \\left[ \\log p(z_j| x, z_{-j})\\right] }{\\partial q_j(z_j)} = 0$$ \n",
    "\n",
    "$$\\frac{\\partial \\log q^*_j(z_j)}{\\partial z_j} = \\frac{\\partial {E}_{X, Z_{-j}} \\left[ \\log p(z_j| x, z_{-j})\\right] }{\\partial z_j}$$ \n",
    "\n",
    "So finally \n",
    "\n",
    "$$ \\boxed{q^*_j(z_j) \\propto \\exp ({E}_{X, Z_{-j}} \\left[ \\log p(z_j| x, z_{-j})\\right])}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-consolidation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "environmental-things",
   "metadata": {},
   "source": [
    "As $\\tilde p_i(c_i|x)$ be the conditional distribution of $c_i$ given the observations and the other parameters.\n",
    "\n",
    "$$\n",
    "\\tilde p_i(c_i|x) \\propto p(c_i)p(x_i|c_i,\\mu) \\propto p(c_i)\\prod_{k=1}^K \\left(\\varphi_{\\mu_k,1}(x_i)\\right)^{1_{c_i=k}}\\,. \n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\tilde q_{c_i}}[\\log \\tilde p_i(c_i|x)] = \\log p(c_i) + \\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[\\log \\varphi_{\\mu_k,1}(x_i)]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{exp}\\left(\\mathbb{E}_{\\tilde q_{c_i}}[\\log \\tilde p_i(c_i|x)]\\right) &\\propto p(c_i) \\mathrm{exp}\\left(\\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[\\log \\varphi_{\\mu_k,1}(x_i)]\\right)\\,\\\\\n",
    "&\\propto p(c_i) \\mathrm{exp}\\left(\\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[-(x_i-\\mu_k)^2/2]\\right)\\,\\\\\n",
    "&\\propto p(c_i) \\mathrm{exp}\\left(\\sum_{k=1}^K 1_{c_i=k} \\mathbb{E}_{\\tilde q_{c_i}}[-(x_i-\\mu_k)^2/2]\\right)\\,.\n",
    "\\end{align*}\n",
    "\n",
    "The update is then written:\n",
    "\n",
    "$$\n",
    "\\varphi_i(k) \\propto p(c_i=k) \\mathrm{exp}\\left(m_k x_i - \\frac{m^2_k + s_k}{2}\\right)\\,.\n",
    "$$\n",
    "\n",
    "* In dimension m\n",
    "$$\n",
    "q(c_i=k) \\propto p(c_i=k) \\exp(x_i^{T}.m_k - \\frac{m_k^{T}.m_k + \\operatorname{tr}(\\Sigma_{k}) }{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-crazy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "devoted-dodge",
   "metadata": {},
   "source": [
    "As $\\tilde p_k(\\mu_k|x)$ be the conditional distribution of $\\mu_k$ given the observations and the other parameters.\n",
    "\n",
    "$$\n",
    "\\tilde p_k(\\mu_k|x) \\propto p(\\mu_k)\\prod_{i=1}^np(x_i|c_i,\\mu) \\propto p(\\mu_k)\\prod_{i=1}^n p(x_i|\\mu,c_i)\\,. \n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log \\tilde p_k(\\mu_k|x)] = \\log p(\\mu_k) + \\sum_{i=1}^n \\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log p(x_i|\\mu,c_i)]\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{exp}\\left(\\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log \\tilde p_i(c_i|x)]\\right) &\\propto p(\\mu_k) \\mathrm{exp}\\left(\\sum_{i=1}^n\\sum_{k=1}^K  \\mathbb{E}_{\\tilde q_{\\mu_k}}[1_{c_i=k}\\log \\varphi_{\\mu_k,1}(x_i)]\\right)\\,\\\\\n",
    "&\\propto p(\\mu_k) \\mathrm{exp}\\left(\\sum_{i=1}^n \\phi_i(k) \\mathbb{E}_{\\tilde q_{\\mu_k}}[\\log \\varphi_{\\mu_k,1}(x_i)]\\right)\\,\\\\\n",
    "&\\propto \\mathrm{exp}\\left(-\\frac{\\mu_k^2}{2\\sigma^2}-\\frac{1}{2}\\sum_{i=1}^n \\phi_i(k)(x_i-\\mu_k)^2\\right)\\,,\\\\\n",
    "&\\propto \\mathrm{exp}\\left(-\\frac{\\mu_k^2}{2\\sigma^2}+\\sum_{i=1}^n \\phi_i(k)x_i\\mu_k - \\frac{1}{2}\\sum_{i=1}^n \\phi_i(k)\\mu^2_k\\right)\\,.\n",
    "\\end{align*}\n",
    "\n",
    "The update is then written:\n",
    "\n",
    "$$\n",
    "m_k = \\frac{\\sum_{i=1}^n \\phi_i(k)x_i}{1/\\sigma^2 + \\sum_{i=1}^n \\phi_i(k)}\\quad\\mathrm{and}\\quad s_k = \\frac{1}{1/\\sigma^2 + \\sum_{i=1}^n \\phi_i(k)}\\,. \n",
    "$$\n",
    "\n",
    "* In dimension m\n",
    "$$\n",
    "\\Sigma_{k}^{-1} = \\Sigma^{-1} + \\sum_{i=1}^n \\phi_i(k)I_{m}\n",
    "\\quad\\mathrm{and}\\quad m_k = \\Sigma_{k}(\\sum_{i=1}^n \\phi_i(k)x_i)\n",
    "$$\n",
    "\n",
    "For the elbo\n",
    "$$\n",
    "ELBO = \\sum_{i=1}^n \\frac{\\log(\\det(\\Sigma_{k}))-(m_k^{T}\\Sigma^{-1}m_k + \\operatorname{tr}(\\Sigma_{k}\\Sigma^{-1}))}{2} + \\sum_{i=1}^n\\sum_{k=1}^K q_i(k)[m_k^{T}.x_i - \\log(q_i(k)) -  \\frac{m_k^{T}.m_k + \\operatorname{tr}(\\Sigma_{k})}{2} ]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-departure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
